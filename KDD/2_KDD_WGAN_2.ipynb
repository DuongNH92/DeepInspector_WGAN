{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c82169cc",
   "metadata": {},
   "source": [
    "# DNN Critic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5f3b183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    14019\n",
      "1      506\n",
      "Name: Label, dtype: int64\n",
      "Counter({0.0: 9809, 1.0: 358})\n",
      "Counter({0.0: 4210, 1.0: 148})\n",
      "Building Critic WGAN model...\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-03 14:56:00.050497: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 2s 6ms/step - loss: 0.1625 - accuracy: 0.9647 - val_loss: 0.0680 - val_accuracy: 0.9660\n",
      "Epoch 2/30\n",
      "102/102 [==============================] - 1s 6ms/step - loss: 0.0487 - accuracy: 0.9856 - val_loss: 0.0346 - val_accuracy: 0.9911\n",
      "Epoch 3/30\n",
      "102/102 [==============================] - 1s 7ms/step - loss: 0.0353 - accuracy: 0.9917 - val_loss: 0.0294 - val_accuracy: 0.9920\n",
      "Epoch 4/30\n",
      "102/102 [==============================] - 1s 5ms/step - loss: 0.0308 - accuracy: 0.9925 - val_loss: 0.0258 - val_accuracy: 0.9931\n",
      "Epoch 5/30\n",
      "102/102 [==============================] - 1s 7ms/step - loss: 0.0273 - accuracy: 0.9928 - val_loss: 0.0237 - val_accuracy: 0.9940\n",
      "Epoch 6/30\n",
      "102/102 [==============================] - 1s 5ms/step - loss: 0.0244 - accuracy: 0.9931 - val_loss: 0.0217 - val_accuracy: 0.9945\n",
      "Epoch 7/30\n",
      "102/102 [==============================] - 1s 7ms/step - loss: 0.0224 - accuracy: 0.9939 - val_loss: 0.0202 - val_accuracy: 0.9945\n",
      "Epoch 8/30\n",
      "102/102 [==============================] - 1s 7ms/step - loss: 0.0208 - accuracy: 0.9942 - val_loss: 0.0203 - val_accuracy: 0.9938\n",
      "Epoch 9/30\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.9940 - val_loss: 0.0185 - val_accuracy: 0.9950\n",
      "Epoch 10/30\n",
      "102/102 [==============================] - 1s 6ms/step - loss: 0.0182 - accuracy: 0.9946 - val_loss: 0.0182 - val_accuracy: 0.9945\n",
      "Epoch 11/30\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.0177 - accuracy: 0.9943 - val_loss: 0.0175 - val_accuracy: 0.9952\n",
      "Epoch 12/30\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.0168 - accuracy: 0.9948 - val_loss: 0.0172 - val_accuracy: 0.9954\n",
      "Epoch 13/30\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.0164 - accuracy: 0.9948 - val_loss: 0.0173 - val_accuracy: 0.9950\n",
      "Epoch 14/30\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.0154 - accuracy: 0.9949 - val_loss: 0.0172 - val_accuracy: 0.9954\n",
      "Epoch 15/30\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.0144 - accuracy: 0.9954 - val_loss: 0.0167 - val_accuracy: 0.9956\n",
      "Epoch 16/30\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9955 - val_loss: 0.0191 - val_accuracy: 0.9959\n",
      "Epoch 17/30\n",
      "102/102 [==============================] - 1s 5ms/step - loss: 0.0134 - accuracy: 0.9953 - val_loss: 0.0153 - val_accuracy: 0.9954\n",
      "Epoch 18/30\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.0123 - accuracy: 0.9957 - val_loss: 0.0154 - val_accuracy: 0.9959\n",
      "Epoch 19/30\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 0.9958 - val_loss: 0.0150 - val_accuracy: 0.9959\n",
      "Epoch 20/30\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.0120 - accuracy: 0.9957 - val_loss: 0.0149 - val_accuracy: 0.9963\n",
      "Epoch 21/30\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9959 - val_loss: 0.0162 - val_accuracy: 0.9959\n",
      "Epoch 22/30\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9964 - val_loss: 0.0157 - val_accuracy: 0.9961\n",
      "Epoch 23/30\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 0.0103 - accuracy: 0.9968 - val_loss: 0.0142 - val_accuracy: 0.9968\n",
      "Epoch 24/30\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 0.0106 - accuracy: 0.9961 - val_loss: 0.0163 - val_accuracy: 0.9961\n",
      "Epoch 25/30\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 0.0099 - accuracy: 0.9964 - val_loss: 0.0140 - val_accuracy: 0.9963\n",
      "Epoch 26/30\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.0091 - accuracy: 0.9967 - val_loss: 0.0133 - val_accuracy: 0.9972\n",
      "Epoch 27/30\n",
      "102/102 [==============================] - 1s 5ms/step - loss: 0.0088 - accuracy: 0.9972 - val_loss: 0.0139 - val_accuracy: 0.9968\n",
      "Epoch 28/30\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 0.0090 - accuracy: 0.9969 - val_loss: 0.0140 - val_accuracy: 0.9966\n",
      "Epoch 29/30\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 0.9971 - val_loss: 0.0137 - val_accuracy: 0.9970\n",
      "Epoch 30/30\n",
      "102/102 [==============================] - 1s 5ms/step - loss: 0.0082 - accuracy: 0.9972 - val_loss: 0.0136 - val_accuracy: 0.9970\n",
      "137/137 [==============================] - 0s 2ms/step\n",
      "Accuracy: 99.70%\n",
      "Saved model!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "from collections import Counter\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# read data\n",
    "mydt = pd.read_csv(\"../dataset/KDD-2018/trainr2l.csv\")\n",
    "print(mydt['Label'].value_counts())\n",
    "\n",
    "# Create matrix of features and matrix of target variable \n",
    "dataset = mydt.values\n",
    "X = dataset[:,0:41]\n",
    "y = dataset[:,41]\n",
    "\n",
    "# convert the scaled array to dataframe \n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)\n",
    "\n",
    "X_full = X_scale\n",
    "y_full = y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.3, random_state=1)\n",
    "\n",
    "print(Counter(y_train))\n",
    "print(Counter(y_test))\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(41,), activation='relu'))\n",
    "model.add(Dense(41, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(\"Building Critic WGAN model...\")\n",
    "# fit the keras model on the dataset\n",
    "mod = model.fit(X_train, y_train, epochs=30, batch_size=100, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# make class predictions with the model\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100))\n",
    "\n",
    "# Save model\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"DNN_critic_model_KDD_2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"DNN_critic_model_KDD_2.h5\")\n",
    "print(\"Saved model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa13502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix for test data\n",
      " [[4203    7]\n",
      " [   6  142]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4210\n",
      "           1       0.95      0.96      0.96       148\n",
      "\n",
      "    accuracy                           1.00      4358\n",
      "   macro avg       0.98      0.98      0.98      4358\n",
      "weighted avg       1.00      1.00      1.00      4358\n",
      "\n",
      "Accuracy: 0.9970\n",
      "Precision: 0.9530\n",
      "Recall: 0.9595\n",
      "F1 Score: 0.9562\n",
      "FPR Score: 0.0405\n"
     ]
    }
   ],
   "source": [
    "# Testing \n",
    "cm = confusion_matrix(y_test, y_pred) \n",
    "tnr = recall_score(y_test, y_pred) \n",
    "fpr = 1 - tnr\n",
    "\n",
    "print (\"confusion_matrix for test data\\n\",cm)\n",
    "print(classification_report(y_test,y_pred,labels=np.unique(y_pred)))\n",
    "print('Accuracy: %.4f' % accuracy_score(y_test, y_pred))\n",
    "print('Precision: %.4f' % precision_score(y_test, y_pred))\n",
    "print('Recall: %.4f' % recall_score(y_test, y_pred))\n",
    "print('F1 Score: %.4f' % f1_score(y_test, y_pred))\n",
    "print('FPR Score: %.4f' % fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c46926",
   "metadata": {},
   "source": [
    "# WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0a63c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn\n",
    "import torch.nn.functional as nn \n",
    "import torch.autograd as autograd \n",
    "import torch.optim as optim \n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.metrics import confusion_matrix \n",
    "import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from keras import optimizers\n",
    "from keras.models import model_from_json\n",
    "import matplotlib \n",
    "matplotlib.use('agg') \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.gridspec as gridspec \n",
    "\n",
    "# hyperparameters \n",
    "mb_size = 64 \n",
    "z_dim = 100 \n",
    "X_dim = 41 \n",
    "y_dim = 1 \n",
    "h_dim = 128 #hidden \n",
    "lr = 0.00005 #learning_rate\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv(\"../dataset/KDD-2018/trainr2l.csv\")\n",
    "print('original data label features number:')\n",
    "print(df['Label'].value_counts())\n",
    "\n",
    "# data preparation\n",
    "criteria0 = df['Label'] == 1 #filter attack data \n",
    "data0 = df[criteria0]\n",
    "\n",
    "float_array = data0.values.astype(float) \n",
    "scaler = MinMaxScaler()\n",
    "scaled_array = scaler.fit_transform(float_array)\n",
    "attack_data = pd.DataFrame(scaled_array)\n",
    "attack_data.columns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes',\n",
    "       'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',\n",
    "       'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',\n",
    "       'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n",
    "       'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "       'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
    "       'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
    "       'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
    "       'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
    "       'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "       'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
    "       'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "       'dst_host_srv_rerror_rate', 'Label']\n",
    "\n",
    "print('----------------------------------------') \n",
    "print('attack value number:') \n",
    "print(attack_data['Label'].value_counts())\n",
    "\n",
    "# transform data to tensor format\n",
    "train_target = torch.tensor(attack_data['Label'].values.astype(np.float32)) \n",
    "train = torch.tensor(attack_data.drop(['Label'], axis = 1).values.astype(np.float32)) \n",
    "train_tensor = Data.TensorDataset(train, train_target) \n",
    "\n",
    "#creat DataLoader with batch and shuffle\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = mb_size, shuffle = True)\n",
    "\n",
    "# define models\n",
    "# Generator\n",
    "G = torch.nn.Sequential(\n",
    "    torch.nn.Linear(z_dim, h_dim), \n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h_dim, h_dim), \n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h_dim, X_dim), \n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Discriminator\n",
    "D = torch.nn.Sequential( \n",
    "    torch.nn.Linear(X_dim, h_dim), \n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h_dim, h_dim), \n",
    "    torch.nn.ReLU(),\n",
    "    # No sigmoid\n",
    "    torch.nn.Linear(h_dim, 1),\n",
    ")\n",
    "\n",
    "# setup cpu using cpu (if use gpu -> set cpu to cuda)\n",
    "G.cpu()\n",
    "D.cpu()\n",
    "\n",
    "def reset_grad():\n",
    "    G.zero_grad()\n",
    "    D.zero_grad()\n",
    "    \n",
    "#initialize optimizers and dataholders\n",
    "G_solver = optim.RMSprop(G.parameters(), lr=lr)\n",
    "D_solver = optim.RMSprop(D.parameters(), lr=lr)\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "data_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef51d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training \n",
    "print(\"Start training WGAN model...\")\n",
    "d_iter = 2 \n",
    "for it in range(500000):\n",
    "    if it<100: d_iter = 50 \n",
    "    else:\n",
    "        d_iter = 2\n",
    "    for _ in range(d_iter):\n",
    "        # Sample data\n",
    "        z = Variable(torch.randn(mb_size, z_dim)).cpu()\n",
    "        try:\n",
    "            X, _ = data_iter.next() \n",
    "        except StopIteration:\n",
    "            data_iter = iter(train_loader) \n",
    "            X, _ = data_iter.next()\n",
    "        #X = Variable(torch.from_numpy(X))\n",
    "        X = X.cpu()\n",
    "        \n",
    "        # Dicriminator forward-loss-backward-update \n",
    "        G_sample = G(z)\n",
    "        D_real = D(X)\n",
    "        D_fake = D(G_sample)\n",
    "        \n",
    "        D_loss = -(torch.mean(D_real) - torch.mean(D_fake))\n",
    "        \n",
    "        D_loss.backward() \n",
    "        D_solver.step()\n",
    "        \n",
    "        # Weight clipping \n",
    "        for p in D.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "            \n",
    "        D_losses.append(D_loss.data.cpu().numpy())\n",
    "        \n",
    "        # Housekeeping - reset gradient \n",
    "        reset_grad()\n",
    "        \n",
    "    # Generator forward-loss-backward-update \n",
    "    #X = Variable(torch.from_numpy(X)) \n",
    "    z = Variable(torch.randn(mb_size, z_dim)).cpu() #change cpu to cuda\n",
    "    \n",
    "    G_sample = G(z)\n",
    "    D_fake = D(G_sample)\n",
    "    \n",
    "    G_loss = -torch.mean(D_fake)\n",
    "    \n",
    "    G_loss.backward() \n",
    "    G_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient \n",
    "    reset_grad()\n",
    "\n",
    "    G_losses.append(G_loss.data.cpu().numpy())\n",
    "\n",
    "    # Print and plot every now and then \n",
    "    if it % 1000 == 0:\n",
    "        print('Iter-{}; D_loss: {}; G_loss: {}'\n",
    "              .format(it, D_loss.data.cpu().numpy(),  \n",
    "                G_loss.data.cpu().numpy()))\n",
    "        \n",
    "        samples = G(z).data.numpy()[:16]\n",
    "\n",
    "#plot losses\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Critic Loss During Training\") \n",
    "plt.plot(G_losses,label=\"Generator\") \n",
    "plt.plot(D_losses,label=\"Critic\") \n",
    "plt.xlabel(\"iter\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.savefig('KDD_loss_2.png')\n",
    "\n",
    "#save model\n",
    "torch.save({'modelG_state_dict': G.state_dict(), \n",
    "            'modelD_state_dict': D.state_dict(),\n",
    "            'optimizerG_state_dict': G_solver.state_dict(),\n",
    "            'optimizerD_state_dict': D_solver.state_dict(),\n",
    "            'G_loss': G_losses,\n",
    "            'D_loss': D_losses\n",
    "           }, \"WGAN_model_KDD_2.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947cefd",
   "metadata": {},
   "source": [
    "# Data generation from saved WGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2da982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loaded NN model\n",
    "print(\"Loading model...\")\n",
    "print(\"----------------------------------------\")\n",
    "json_file = open(\"DNN_critic_model_KDD_2.json\",\"r\") \n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json) \n",
    "loaded_model.load_weights(\"DNN_critic_model_KDD_2.h5\")\n",
    "\n",
    "G = torch.nn.Sequential(\n",
    "    torch.nn.Linear(z_dim, h_dim),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h_dim, h_dim),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h_dim, X_dim), \n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(\"WGAN_model_KDD_2.pth\")\n",
    "G.load_state_dict(checkpoint['modelG_state_dict'])\n",
    "\n",
    "G.eval()\n",
    "G.cpu()\n",
    "\n",
    "final_Data = pd.DataFrame(columns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes',\n",
    "       'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',\n",
    "       'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',\n",
    "       'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n",
    "       'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "       'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
    "       'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
    "       'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
    "       'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
    "       'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "       'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
    "       'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "       'dst_host_srv_rerror_rate', 'Label']) \n",
    "print(\"empty df\",final_Data) \n",
    "print(\"----------------------------------------\")\n",
    "print(\"Completed loading model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73faade2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "it =0\n",
    "\n",
    "while(final_Data.shape[0]<20000):\n",
    "    it+=1\n",
    "    z = Variable(torch.randn(100, z_dim)).cpu() \n",
    "    with torch.no_grad():\n",
    "        data = G(z)\n",
    "    data = data.cpu().numpy() \n",
    "    fake = pd.DataFrame(data)\n",
    "        \n",
    "    newy = np.ones((fake.shape[0],)) \n",
    "    #this is the model which changes \n",
    "    new_pred = loaded_model.predict(fake)\n",
    "    new_pred = (new_pred > 0.5)\n",
    "    cm = confusion_matrix(newy,new_pred)\n",
    "    print(\"-------------------------------------------------------------\") \n",
    "    print (\" confusion_matrix for new data with deeper model \\n\" ,cm)\n",
    "    \n",
    "    combined_data = np.concatenate((fake,new_pred),axis=1) \n",
    "    combined_data = pd.DataFrame(combined_data)\n",
    "    combined_data.columns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes',\n",
    "       'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',\n",
    "       'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',\n",
    "       'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n",
    "       'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "       'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
    "       'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
    "       'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
    "       'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
    "       'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "       'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
    "       'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "       'dst_host_srv_rerror_rate', 'Label']\n",
    "    \n",
    "    criteria1 = combined_data['Label'] == 1 #just attack data \n",
    "    data1 = combined_data[criteria1]\n",
    "    print(\"only attack data - \",data1.shape) \n",
    "    final_Data = final_Data.append(data1, ignore_index = True) \n",
    "    print(\"final data \", it, \"- \",final_Data.shape) \n",
    "    print(\"-------------------------------------------------------------\") \n",
    "    \n",
    "print(final_Data['Label'].value_counts())\n",
    "print(\"---------------------------------------------------------\")\n",
    "df2 = final_Data.sample(n=13513, replace=False, random_state=1)\n",
    "df3 = final_Data.sample(n=19494, replace=False, random_state=1)\n",
    "final = pd.concat([mydt,df2], ignore_index = True,axis=0)\n",
    "final['Label'].value_counts()\n",
    "df3.to_csv(\"../dataset/KDD-2018/generated_r2l.csv\", index = False, header = True)\n",
    "final.to_csv(\"../dataset/KDD-2018/balanced_data_r2l.csv\", index = False, header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
